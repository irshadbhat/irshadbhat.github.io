---
layout: post
title: Google Summer of Code 2016 (GSoC)
permalink: /gsoc/
---

I feel privileged to have been given the opportunity to work for [Libindic organization](https://github.com/libindic) under the Google Summer of Code 2016. Libindic is an open source library that supports many utilities for text processing of Indian languages. I would be contributing towards the automatic script transliteration between scheduled languages of India including English. For the project, I would be mentored by [Riyaz Ahmad Bhat](https://researchweb.iiit.ac.in/~riyaz.bhat/) and [Santhosh Thottingal](http://thottingal.in).

# Machine Transliteration
[Transliteration](https://en.wikipedia.org/wiki/Transliteration) simply means conversion of a text from one script to another. Machine transliteration is the computer automated process of transcribing a character or word from one language script to another. Machine transliteration can play an important role in natural language application such as information retrieval and machine translation, especially for handling proper nouns and technical terms, cross-language applications, data mining and information retrieval system. 

## Project Description
A thorough description of the project goals can be seen in the original [proposal](https://drive.google.com/open?id=0B3ZAq0KmDeDVd3N3TnluS2tCLWM) submitted to GSoC 2016. Here I will briefly list down our contribution towards machine transliteration module of Libindic.

* Indic to Indic Script Transliteration
* Indic to Roman Transliteration

## Approaches

### [1. Rule Based](https://en.wikipedia.org/wiki/Rule-based_system)

A rule-based transliteration system uses character mappings defined between two scripts. There are five types of character mappings possible between two natural language scripts: *One-to-One*, *One-to-Many*, *Many-to-One*, *One-to-None* and *None-to-One*. A simple *One-to-One* character mapping would lead to a more accurate and easy to develop system. However, natural languages are inherently ambiguous. There are many cases of ambiguity in orthographic representation of languages world over. Distribution of different types of character mappings is not generally uniform, though. The distribution is usually skewed; unambiguous characters occur more often then their ambiguous counterparts. Nonetheless, to resolve the ambiguous mappings, heuristics are designed which take the surrounding context of an ambiguous letter into consideration.

* #### Benefits
 1. Rule-based transliteration systems between Indic scripts is essentially trivial. Indic scripts have a special property that their phonemes are one-to-one aligned between their Unicode tables. It essentially means that the transliteration between any two Indic scripts can be achieved by merely using their unicode tables.
 2. Rule-based systems do not require any kind of training data to develop the system.

* #### Challenges
 1. ***Requires domain expertise***: To develop a rule-based system between two natural language scripts a domain expert is required. 
 2. ***Missing phonemes in Indic scripts***: The major concern with rule-based system for Indic-to-Indic transliteration is the missing phonemes in Indic scripts. For example, in Tamil there are no characters for *d*, *dh*, *b*, *bh* etc. *d* is pronounced as *t*, *dh* as *th*, *b* and *bh* are pronounced as *p*. Similarly, in Bengali there is no character for *v*, it is pronounced as *b*.
 3. ***Ambiguous Character Mappings***: Another major issue with rule-based system for is ambiguous character mappings between two natural language scripts. For example, Tamil has the following ambiguous characters:

    <p>
    <center>
    <table class="table-fill">
    <tr>
    <th><b>&emsp;Character&emsp;</b></th> <th><b>&emsp;&emsp;Mapping&emsp;&emsp;</b></th>
    </tr>
    <tr>
    <td>k</td> <td>k, kh, g, gh, h</td>
    </tr>
    <tr>
    <td>c</td> <td>c, ch, j, jh, s</td>
    </tr>
    <tr>
    <td>t</td> <td>t, th, d, dh</td>
    </tr>
    <tr>
    <td>T</td> <td>T, Th, D, Dh</td>
    </tr>
    <tr>
    <td>p</td> <td>p, ph, b, bh</td>
    </tr>
    </table>
    </center>
    </p>

### [2. Machine Learning (ML)](https://en.wikipedia.org/wiki/Machine_learning) 
Arthur Samuel defined machine learning as a "Field of study that gives computers the ability to learn without being explicitly programmed". ML is a method of teaching computers to make and improve predictions or behaviors based on some training data. In case of transliteration, the training data are the transliteration pairs between the language scripts to be transliterated. 

* #### Benefits
 1. A major benefit with transliteration system is that one is not required to be a domain expert of all the languages.
 2. ML systems are compact, more generic and less hectic to develop than the rule-based ones. 

* #### Challenges
 1. A strong list of transliteration pairs is required to train the model. However, such lists are not readily available and are expensive to create manually.
 2. Selection of proper ML technique, data representation for training the models are some minor challenges. 

## During Community Bonding
During the community bonding period I explored some of the existing tools on Indic-transliteration. Created different test cases to evaluate these existing systems, to gauge the weaknesses and strengths of these systems.

## Progress So Far

### Development of training data for Indic-Roman transliteration systems
Since I am using ML approach to develop Indic-to-Roman transliteration systems, I need to create the training data first. As mentioned in the proposal, I use the sentence aligned ILCI parallel corpora and Indo-wordnet synsets to extract the transliteration pairs. Initially, the parallel corpus is word-aligned using GIZA++, and the alignments are refined using the grow-diag-final-and heuristic. I extract all word pairs which occur as 1-to-1 alignments in the word-aligned corpus as potential transliteration equivalents. I will explain the transliteration pair extraction process on Hindi-Roman dataset. All the other datasets follow the same procedure.

#### Word alignments with GIZA++
 1. Download and install GIZA++ from [here](http://giza.sourceforge.net/documentation/installation.html).
 2. Run the following bash script to word-align the parallel text files (say ***hin.txt*** and ***eng.txt***):

    ```bash
    src=hin.txt
    trg=eng.txt
    plain2snt.out $src $trg
    mkcls -n10 -p$src -V$trg.vcb.classes
    mkcls -n10 -p$trg -V$src.vcb.classes
    snt2cooc.out $src.vcb $trg.vcb $src"_"$trg".snt" > $src"_"$trg".cooc"
    GIZA++ -ml 101 -S $src.vcb -T $trg.vcb -C $src"_"$trg".snt" -CoocurrenceFile $src"_"$trg".cooc"
    ```

    The resulting word-aligned file (ending with ***A3.final***) will look like this:

    ```
    # Sentence pair (1) source length 6 target length 6 alignment score : 2.28771e-11
    लगातार बुखार से पीड़ित हो ?
    NULL ({ }) I ({ }) suffer ({ 4 5 }) from ({ 3 }) fever ({ 2 }) continuously ({ 1 }) . ({ 6 })
    # Sentence pair (2) source length 10 target length 11 alignment score : 2.70124e-12
    कालाजार , मलेरिया या फिर हाई फीवर हो सकता है ।
    NULL ({ }) Kalajar ({ 1 }) , ({ 2 }) Malaria ({ 3 }) or ({ 4 }) high ({ }) fiver ({ 5 6 7 }) may ({ 9 }) have ({ 10 }) happened ({ 8 }) . ({ 11 })
    # Sentence pair (3) source length 8 target length 9 alignment score : 8.33816e-16
    हर छह माह में करें आँखों की जाँच ।
    NULL ({ 4 }) Get ({ 5 }) eyes ({ 6 }) checked ({ 7 8 }) up ({ }) every ({ 1 }) six ({ 2 }) months ({ 3 }) . ({ 9 })
    # Sentence pair (4) source length 4 target length 4 alignment score : 1.89822e-07
    नियमित आँखें धोना ।
    NULL ({ }) Washing ({ 3 }) eyes ({ 2 }) regularly ({ 1 }) . ({ 4 })
    .
    .
    .
    ```

    These alignments can be further improved using the [grow-diag-final-and](http://www.statmt.org/moses/?n=FactoredTraining.AlignWords) heuristic. Note that the numerals in curly braces are the indices of Hindi words aligned to English words. From this alignment file the one-to-one alignments are considered as the potential transliteration equivalents. To further augment the transliteration pairs I also added word pairs from Hindi-wordnet synset mappings. 

#### Removal of Noisy Pairs
Obviously there will be a lot of word pairs which are not transliteration equivalents rather are translation pairs or noisy alignments. To filter out the noisy word pairs I use the [Levenshtein edit distance](https://en.wikipedia.org/wiki/Levenshtein_distance) measure. In order to compute the edit distances we need a character mapping table between the two scripts. A sample mapping table between Hindi and Roman scripts is shown below:

<p>
<center>
<table class="table-fill">
<tr>
  <th colspan="8" >Hindi-Roman Mapping Table</th>
</tr>
<tr>
<td>अ</td>  <td>a,e</td>  <td>ल</td>  <td>l</td>  <td>त</td>  <td>t,th</td>  <td>झ</td>  <td>jh</td>
</tr>
<tr>
<td>ब</td>  <td>b</td>  <td>म</td>  <td>m</td>  <td>द</td>  <td>d,dh</td>  <td>ख</td>  <td>kh</td>
</tr>
<tr>
<td>च</td>  <td>c,ch</td>  <td>न</td>  <td>n</td>  <td>य</td>  <td>y,i,e</td>  <td>ण</td>  <td>n</td>
</tr>
<tr>
<td>ड</td>  <td>d,dh</td>  <td>ओ</td>  <td>o,u</td>  <td>आ</td>  <td>a,aa</td>  <td>औ</td>  <td>o,u</td>
</tr>
<tr>
<td>ए</td>  <td>e,i,y,a</td>  <td>प</td>  <td>p</td>  <td>भ</td>  <td>bh</td>  <td>फ</td>  <td>ph,f</td>
</tr>
<tr>
<td>ङ</td>  <td>n</td>  <td>ऋ</td>  <td>r,ri,ru</td>  <td>छ</td>  <td>ch</td>  <td>ष</td>  <td>sh</td>
</tr>
<tr>
<td>ग</td>  <td>g,gh</td>  <td>र</td>  <td>r</td>  <td>ढ</td>  <td>dh</td>  <td>श</td>  <td>sh</td>
</tr>
<tr>
<td>ह</td>  <td>h,gh</td>  <td>स</td>  <td>s,c</td>  <td>ऐ</td>  <td>e,i,y</td>  <td>ठ</td>  <td>th</td>
</tr>
<tr>
<td>इ</td>  <td>e,i,y</td>  <td>ट</td>  <td>t,th</td>  <td>ञ</td>  <td>n</td>  <td>ऊ</td>  <td>o,u</td>
</tr>
<tr>
<td>ज</td>  <td>j,jh,z,g</td>  <td>उ</td>  <td>o,u</td>  <td>घ</td>  <td>gh</td>  <td>थ</td>  <td>th</td>
</tr>
<tr>
<td>क</td>  <td>k,q,c</td>  <td>व</td>  <td>v,w</td>  <td>ई</td>  <td>e,i,y</td>  <td>ध</td>  <td>dh</td>
</tr>
</table>
</center>
</p>

A straightforward python implementation for a function LevenshteinDistance that takes two strings, ***src*** of length m, and ***trg*** of length n, and a mapping table ***map_table***, and returns the Levenshtein distance between the two strings is given below:

```python
import numpy as np
def LevenshteinDistance(src, trg, map_table):
  m = len(src) + 1
  n = len(trg) + 1
  d = np.zeros((m, n), np.uint16)
  # source prefixes can be transformed into empty string by
  # dropping all characters
  d[:, 0] = range(m)
  # target prefixes can be reached from empty source prefix
  # by inserting every character
  d[0] = range(n)
  for j in range(1, n):
    for i in range(1, m):
      if trg[j-1] in map_table[src[i-1]]:
        d[i, j] = d[i-1, j-1]
      else:
        d[i, j] = min(d[i-1, j] + 1,    #// deletion
                      d[i, j-1] + 1,    #// insertion
                      d[i-1, j-1] + 1)  #// substitution
  return d[m-1, n-1]
```

Note that ***map_table*** is a python dictionary with Hindi letters as keys and their Roman mappings as values. The edit-distance returned is bounded between 0 to the length of largest string (source and target). These edit-distance scores are normalized between 0 to 1 simply by dividing the returned score with the length of the largest string. An edit-distance score of 1 between two strings with max-string length of 2 has a normalized edit-distance of 0.5, whereas the same score between two strings with max-string length of 10 has a normalized edit distance of 0.1. 
Translation pairs with a normalized score of less than a small threshold of **~0.3** are considered as transliteration pairs. Note that the threshold is chosen by a rough observation of translation pairs and can vary for different language pairs.

#### Character alignments with GIZA++
In order to train the transliteration system, we need to character align the transliteration pairs obtained above. We again use GIZA++ for this task. Let Hindi strings be the source and their Roman transliterations be the target for GIZA++ alignments. The resulting character alignments obtained using GIZA++ will look like:

```
# Sentence pair (1) source length 6 target length 10 alignment score : 3.91859e-13
m i s s i o n a r y
NULL ({ 5 6 8 }) म ({ 1 })  ि ({ 2 }) श ({ 3 4 }) न ({ 7 }) र ({ 9 }) ी ({ 10 })
# Sentence pair (2) source length 8 target length 6 alignment score : 1.5955e-05
u n i t e d
NULL ({ }) य ({ }) ू ({ 1 }) न ({ 2 }) ा ({ }) इ ({ 3 }) ट ({ 4 }) े ({ 5 }) ड ({ 6 })
# Sentence pair (3) source length 6 target length 8 alignment score : 9.23754e-05
a i sh w a r y a
NULL ({ 1 5 }) ऐ ({ 2 }) श ({ 3 }) व् ({ 4 }) र ({ 6 }) य ({ 7 }) ा ({ 8 })
# Sentence pair (4) source length 6 target length 8 alignment score : 0.0111937
n a r e n d r a
NULL ({ 2 8 }) न ({ 1 }) र ({ 3 }) े ({ 4 }) ं ({ 5 }) द ({ 6 }) र् ({ 7 })

.
.
.
```

Giza++ produces  four types of alignments: 1-to-1, 1-to-Many, 1-to-None and None-to-1. Out of these four letter alignments, None-to-1 alignments need to be handled. These alignments are modified by merging the target character with the previous aligned pair if it is not the first character, otherwise it is merged with the succeeding aligned character pair. The final training data for Hindi-to-Roman transliteration system will look like:

```
म   m
ि   i
श   ssio
न   na
र   r
ी   y

य   _
ू   u
न   n
ा   _
इ   i
ट   t
े   e
ड   d

ऐ   ai
श   sh
व्   wa
र   r
य   y
ा   a

न   na
र   r
े   e
ं   n
द   d
र्   ra
v lang="latex">
\frac{1+sin(x)}{y}
</div>
.
.
.
```

In order to create training data for Roman-to-Hindi system, we have to pass Roman strings as source and their Hindi transliterations as target to the character alignment process.

### Training the Transliteration System

Now that we have the training data, we can start the final training process. We can apply any ML algorithm to train the system. I model transliteration as a structure prediction problem with global feature representation. My transliteration model is basically a second order Hidden Markov Models (SHMM) formally represented in Equation 1. I denote the sequence of letters in a word in source script as boldface **s** and the sequence of hidden states which correspond to letter sequences in the target script as boldface **t**. A basic HMM model has the following parameters:

<center>
<div lang="latex">
P(\b{s};\b{t}) = \underset{t_{1}...t_{n}}{\arg\max}
{\overset{n}{\underset{i=1}{\prod}}}
\underbrace{P(t_{i}|t_{i-1},t_{i-2}) \rule[-5pt]{0pt}{1pt}}_{\mbox{\tiny Transition Probabilities}}
\underbrace{P(s_{i}|t_{i}) \rule[-5pt]{0pt}{1pt}}_{\mbox{\tiny Emission Probabilities}}
</div>
    
<div lang="latex">
\text{where }{$s_{i}...s_{n}$} \text{ is a letter sequence in the source script, and } \\ {$t_{i}...t_{n}$} \text{ is the corresponding letter sequence in the target script.}
</div>
</center>

Instead of maximum likelihood estimates, I use [structured perceptron of Collins](http://www.aclweb.org/anthology/W02-1001) to learn the model parameters. With structured perceptron local contextual features can be made relevant to the whole sequence of target letters. It also allows us to use feature based emissions. I replace the basic multinomial emissions <span lang="latex">P(s$_{i}$|t$_{i}$)</span> with the feature based emissions <div lang="latex">($\theta$ $\cdot$ f(s,t))</div>; where *f(s,t)* is a feature function and \(\theta\) are the model parameters. The feature template used to learn the emissions is shown in Table (\ref{tbl:trans}). I use second-order viterbi to decode the best letter sequence in the target script while learning the parameters as well as at the time of testing.


[comment]: <> I will list down the progress on the project in three major blocks of time.

[comment]: <> ## Towards  Mid-term Evaluation

[comment]: <> ## Towards Final Evaluation
[comment]: <> coming soon .. .. ..
