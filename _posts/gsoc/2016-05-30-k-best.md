---
layout: post
title: K-Best Transliterations
permalink: /k-best/
category: gsoc
data: 2016-06-28
tags: 
- libindic
- gsoc 2016
- machine transliteration
- machine learning
- Indian languages
- structured perceptron
- beam search
- viterbi search
---

No matter how amazing the ML approach used to model a problem, it can never guarantee 100% results or even close. The accuracy of the model mostly depends on th number of training samples, the material impact of training samples on the variable being predicted and ML algorithm selected to train the model. We can only approach the best possible results by selecting a proper ML technique which itself depends on the problem definition. Machine transliteration has many applications like information retrieval, machine translation (handling named-entities), cross-language applications, information retrieval systems, input tools etc. Some of these applications can benefit if we can generate *k-best* transliterations for an input string rather than a single one. For example, in machine translation if we generate some *k* transliterations for a named-entity, we can select the best transliteration for the word using any decoding technique like viterbi, A* search etc. In case we use transliteration for input tools, it would highly benefit the user if we provide some *k-best* transliterations, so the user can pick one that suits the best.

## Viterbi Decoding

So far the machine transliteration system returns a single best output by applying ***viterbi decoding*** (exact search) on the learned model parameters (emission and transition matrices). The Viterbi algorithm requires knowledge of the parameters of the HMM model and given an observation sequence (source string in this case), it finds the state sequence (target string) that is most likely to have generated that observation sequence. It works by finding a maximum over all possible state sequences using the following two steps:

 * Forward step, calculate path to the best state given all the previously observed states and observations.
 * Backward step, reproduce the path.

Can we use Viterbi search for selecting k-best output sequences? The answer is no. The reason lies in the forward step of the algorithm, viterbi does not keep track of the 2nd best, 3rd best or in general nth best state given previously observed states and observations. So we can not reproduce the n-best paths. 

## Beamsearch decoding

Even though beamsearch is most often used to maintain tractability in large systems with insufficient amount of memory to store the entire search tree, it can also be used to select the k-best output sequences given an HMM model and the observation sequence. Beamsearch is exactly like viterbi search except that we keep track of only the k-best hypotheses at each step and drop the remaining low scoring hypotheses. This allows us to reproduce all these k-best paths.

```python
from indictrans import transliterator
r2i = transliterator(
                    source='eng',
                    target='hin',
                    decode='beamsearch',
                    k_best=5)

r2i.transform('indictrans')
['इंडिक्ट्रांस', 'इंडिक्ट्राँस', 'इंडिक्ट्रास', 'इंडिक्ट्रान्स', 'इण्डिक्ट्रांस']

r2i.transform('libindic')
['लिबिंदिक', 'लीबिंदिक', 'लिबिन्दिक', 'लिबिंदिय', 'लिबिंदिक्']

r2i.transform('dilchasp')
['दिल्चस्प', 'दिलचस्प', 'दिल्चेस्प', 'दिलचेस्प', 'दिल्चसप']

r2i.transform('shruti')
['श्रुति', 'श्रुती', 'शरुति', 'श्युति', 'श्रुथि']
```
